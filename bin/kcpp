#!/usr/bin/env python3
# author: daniel rode
# dependencies: podman, podman-compose
# created: 01 dec 2025
# updated: 01 feb 2026


# Run LLMs (AI chat) locally via Koboldcpp.


import secrets
import subprocess as sp
from pathlib import Path

import yaml


MODEL_URL = (
    # 'https://huggingface.co/tiiuae/Falcon3-3B-Instruct-GGUF/resolve/main/'
    #     'Falcon3-3B-Instruct-q4_0.gguf'
    'https://huggingface.co/unsloth/gemma-3-1b-it-GGUF/resolve/main/'
        'gemma-3-1b-it-BF16.gguf'
    # 'https://huggingface.co/unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF/resolve/'
    #     'main/Qwen3-Coder-30B-A3B-Instruct-UD-TQ1_0.gguf'
    # 'https://huggingface.co/Manojb/Qwen3-4B-toolcalling-gguf-codex/resolve/'
    #     'main/Qwen3-4B-Function-Calling-Pro.gguf'
    # 'https://huggingface.co/unsloth/GLM-4.7-Flash-GGUF/resolve/main/'
    #     'GLM-4.7-Flash-UD-Q2_K_XL.gguf'
)
MODEL_FILENAME = MODEL_URL.split('/')[-1]

MODEL_CONTEXT = 2**17

HOME = Path.home()
PW_PATH = HOME / ".appdata/kcpp/creds.txt"
GGUF_MODEL_DIR = HOME / ".appdata/kcpp/gguf"


def get_con_yaml(admin_pw):
    return yaml.dump({
        'version': 3.2,
        'services': {
            'koboldcpp': {
                'image': 'docker.io/koboldai/koboldcpp:latest',
                'devices': [
                    '/dev/dri',
                ],
                'volumes': [
                    '~/.appdata/kcpp/con_vol:/workspace:z',
                    '~/.appdata/kcpp/gguf:/gguf:ro,z',
                ],
                'environment': [
                    'KCPP_DONT_UPDATE=flase',
                    'KCPP_DONT_TUNNEL=true',
                    'KCPP_ARGS=' + ' '.join([
                        f'--model=/gguf/{MODEL_FILENAME}',
                        '--usevulkan',

                        # "The more layers you offload to VRAM, the faster
                        # generation speed will become. Experiment to determine
                        # number of layers to offload, and reduce by a few if
                        # you run out of memory."
                        '--gpulayers=99',

                        '--multiuser=20',
                        '--admin',
                        '--admindir=.',
                        f'--adminpassword={admin_pw}',
                        f'--contextsize={MODEL_CONTEXT}',
                    ]),
                ],
                'ports': [
                    '127.0.0.1:5001:5001',
                ],
                'restart': 'unless-stopped',
            },
        },
    })


# Main

# Write default credentials file for kcpp, if needed
if not PW_PATH.exists():
    print("Koboldcpp credential file not found; creating...")
    PW_PATH.parent.mkdir(parents=True, exist_ok=True)
    with PW_PATH.open('w') as f:
        f.write(f"ADMIN_PW={secrets.token_urlsafe(32)}\n")
        f.write(f"GGUF_MODEL={MODEL_FILENAME}\n")
        f.write("CONTEXT_SIZE=131072\n")

admin_pw = PW_PATH.read_text().strip()

# Download model, if needed
MODEL_PATH = GGUF_MODEL_DIR / MODEL_FILENAME
if not MODEL_PATH.exists():
    print("Model file not found; downloading...")
    GGUF_MODEL_DIR.mkdir(exist_ok=True, parents=True)
    cmd = (
        'wget',
        '--output-document', str(MODEL_PATH),
        MODEL_URL,
    )
    sp.run(cmd, check=True)

# Ensure appdata dir exists for kcpp
Path(HOME, ".appdata/kcpp/").mkdir(parents=True, exist_ok=True)

# Run koboldcpp
cmd = (
    'podman-compose',
    '--file', '-',
    'up',
)
sp.run(cmd, input=get_con_yaml(admin_pw), text=True)



# TODO glxinfo | rg -u -i mem
