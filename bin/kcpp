#!/usr/bin/env python3
# author: daniel rode
# dependencies: podman, podman-compose
# created: 01 dec 2025
# updated: 10 feb 2026


# Run LLMs (AI chat) locally via Koboldcpp.
#
# ssh -tL 5001:localhost:5001 REMOTE_IP /home/daniel/code/bin/kcpp Qwen2.5-7B-Instruct-Q4_0.gguf


import sys
import secrets
import subprocess as sp
from pathlib import Path

import yaml


DEFAULT_MODEL = 'gemma-3-1b-it-BF16.gguf'
MODEL_URL_MAP = {i.split('/')[-1]:i for i in (
    # Useful model list: https://huggingface.co/bartowski
    'https://huggingface.co/tiiuae/Falcon3-3B-Instruct-GGUF/resolve/main/Falcon3-3B-Instruct-q4_0.gguf',
    'https://huggingface.co/unsloth/gemma-3-1b-it-GGUF/resolve/main/gemma-3-1b-it-BF16.gguf',
    'https://huggingface.co/unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF/resolve/main/Qwen3-Coder-30B-A3B-Instruct-UD-TQ1_0.gguf',
    'https://huggingface.co/Manojb/Qwen3-4B-toolcalling-gguf-codex/resolve/main/Qwen3-4B-Function-Calling-Pro.gguf',
    'https://huggingface.co/unsloth/GLM-4.7-Flash-GGUF/resolve/main/GLM-4.7-Flash-UD-Q2_K_XL.gguf',
    'https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-Q8_0.gguf',
    'https://huggingface.co/bartowski/Qwen2.5-7B-Instruct-GGUF/resolve/main/Qwen2.5-7B-Instruct-Q4_0.gguf',
)}

MODEL_CONTEXT = 2**17

HOME = Path.home()
CACHE_DIR = HOME / ".cache"
STATE_DIR = HOME / ".local/share"
KCPP_STATE_DIR = STATE_DIR / "com.github.danielrode/kcpp"

PW_PATH = KCPP_STATE_DIR / "creds.txt"
GGUF_MODEL_DIR = CACHE_DIR / "com.github.danielrode/lcpp/gguf"


def get_con_yaml(admin_pw, model_filename):
    return yaml.dump({
        'version': 3.2,
        'services': {
            'koboldcpp': {
                'image': 'docker.io/koboldai/koboldcpp:latest',
                'devices': [
                    '/dev/dri',
                ],
                'volumes': [
                    str(STATE_DIR) + '/con_vol:/workspace:z',
                    str(GGUF_MODEL_DIR) + ':/gguf:ro,z',
                ],
                'environment': [
                    'KCPP_DONT_UPDATE=true',
                    'KCPP_DONT_TUNNEL=true',
                    'KCPP_ARGS=' + ' '.join([
                        f'--model=/gguf/{model_filename}',
                        '--usevulkan',
                        # '--usecpu',  # Do not use any GPU (CPU Only)
                        # '--multiplayer',  # Hosts a shared multiplayer session that others can join.

                        # "The more layers you offload to VRAM, the faster
                        # generation speed will become. Experiment to determine
                        # number of layers to offload, and reduce by a few if
                        # you run out of memory."
                        '--gpulayers=99',

                        '--skiplauncher',
                        '--multiuser=20',
                        '--admin',
                        '--admindir=.',
                        f'--adminpassword={admin_pw}',
                        f'--contextsize={MODEL_CONTEXT}',
                    ]),
                ],
                'ports': [
                    '127.0.0.1:5001:5001',
                ],
                'restart': 'unless-stopped',
            },
        },
    })


# Main

# Parse command line input
try:
    model_filename = sys.argv[1]
except IndexError:
    model_filename = DEFAULT_MODEL

if model_filename not in MODEL_URL_MAP:
    print("error: Model not found:", model_filename)
    sys.exit(1)

# Write default credentials file for kcpp, if needed
if not PW_PATH.exists():
    print("Koboldcpp credential file not found; creating...")
    PW_PATH.parent.mkdir(parents=True, exist_ok=True)
    with PW_PATH.open('w') as f:
        f.write(f"ADMIN_PW={secrets.token_urlsafe(32)}\n")
        f.write(f"GGUF_MODEL={model_filename}\n")
        f.write("CONTEXT_SIZE=131072\n")

admin_pw = PW_PATH.read_text().strip()

# Download model, if needed
model_path = GGUF_MODEL_DIR / model_filename
if not model_path.exists():
    print("Model file not found; downloading...")
    GGUF_MODEL_DIR.mkdir(exist_ok=True, parents=True)
    cmd = (
        'wget',
        '--output-document', str(model_path),
        MODEL_URL_MAP[model_filename],
    )
    sp.run(cmd, check=True)

# Ensure appdata dir exists for kcpp
KCPP_STATE_DIR.mkdir(parents=True, exist_ok=True)

# Run koboldcpp
cmd = (
    'podman-compose',
    '--file', '-',
    'up',
)
con_yaml = get_con_yaml(admin_pw, model_filename)
sp.run(cmd, input=con_yaml, text=True)
