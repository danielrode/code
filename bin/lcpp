#!/usr/bin/env python3
# author: daniel rode
# dependencies: podman, podman-compose, aria2
# created: 07 feb 2026
# updated: 10 feb 2026


# Run LLMs (AI chat) locally via llama.cpp.
#
# ssh -tL 5001:localhost:5001 REMOTE_IP /home/daniel/code/bin/lcpp Qwen2.5-7B-Instruct-Q4_0.gguf


import sys
import subprocess as sp
from pathlib import Path

import yaml


DEFAULT_MODEL = 'gemma-3-1b-it-BF16.gguf'
MODEL_URL_MAP = {i.split('/')[-1]:i for i in (
    # Useful model list: https://huggingface.co/bartowski
    'https://huggingface.co/tiiuae/Falcon3-3B-Instruct-GGUF/resolve/main/Falcon3-3B-Instruct-q4_0.gguf',
    'https://huggingface.co/unsloth/gemma-3-1b-it-GGUF/resolve/main/gemma-3-1b-it-BF16.gguf',
    'https://huggingface.co/unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF/resolve/main/Qwen3-Coder-30B-A3B-Instruct-UD-TQ1_0.gguf',
    'https://huggingface.co/Manojb/Qwen3-4B-toolcalling-gguf-codex/resolve/main/Qwen3-4B-Function-Calling-Pro.gguf',
    'https://huggingface.co/unsloth/GLM-4.7-Flash-GGUF/resolve/main/GLM-4.7-Flash-UD-Q2_K_XL.gguf',
    'https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF/resolve/main/Ministral-8B-Instruct-2410-Q8_0.gguf',
    'https://huggingface.co/bartowski/Qwen2.5-7B-Instruct-GGUF/resolve/main/Qwen2.5-7B-Instruct-Q4_0.gguf',
)}

MODEL_CONTEXT = 2**17

HOME = Path.home()
CACHE_DIR = HOME / ".cache"
GGUF_MODEL_DIR = CACHE_DIR / "com.github.danielrode/lcpp/gguf"


def get_con_yaml(model_filename, port=5001):
    return yaml.dump({
        'version': 3.2,
        'services': {
            'llamacpp': {
                # 'image': 'ghcr.io/ggml-org/llama.cpp:server',
                'image': 'ghcr.io/ggml-org/llama.cpp:server-vulkan',
                'devices': [
                    '/dev/dri',
                ],
                'volumes': [
                    str(GGUF_MODEL_DIR) + ':/models:ro,z',
                ],
                'environment': [
                    # https://github.com/ggml-org/llama.cpp/blob/master/tools/server/README.md

                    'LLAMA_ARG_WEBUI=true',
                    'LLAMA_ARG_HOST=0.0.0.0',
                    'LLAMA_ARG_PORT=' + str(port),
                    'LLAMA_ARG_MODEL=/models/' + str(model_filename),

                    # Number of tokens to predict (-1 = inf)
                    'LLAMA_ARG_N_PREDICT=-1',

                    # Max number of layers to store in VRAM
                    # 'LLAMA_ARG_N_GPU_LAYERS=64',
                    # 'LLAMA_ARG_N_GPU_LAYERS=99',
                    'LLAMA_ARG_N_GPU_LAYERS=auto',

                    # Size of the prompt context (0 = loaded from model)
                    '--ctx-size' + str(MODEL_CONTEXT),
                ],
                'ports': [
                    f'127.0.0.1:{port}:{port}',
                ],
                'restart': 'unless-stopped',
            },
        },
    })


# Main

# Parse command line input
try:
    model_filename = sys.argv[1]
except IndexError:
    model_filename = DEFAULT_MODEL

if model_filename not in MODEL_URL_MAP:
    print("error: Model not found:", model_filename)
    sys.exit(1)

# Download model, if needed
if not (GGUF_MODEL_DIR / model_filename).exists():
    print("Model file not found; downloading...")
    GGUF_MODEL_DIR.mkdir(exist_ok=True, parents=True)
    cmd = (
        'aria2c',
        '--dir', str(GGUF_MODEL_DIR),
        '--out', str(model_filename),
        MODEL_URL_MAP[model_filename],
    )
    sp.run(cmd, check=True)

# Run llama.cpp
cmd = (
    'podman-compose',
    '--file', '-',
    'up',
)
con_yaml = get_con_yaml(model_filename)
sp.run(cmd, input=con_yaml, text=True)
